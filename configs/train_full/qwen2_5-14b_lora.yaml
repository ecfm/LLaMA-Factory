# Model & training stage
model_name_or_path: Qwen/Qwen2.5-14B-Instruct
stage: sft
do_train: true
finetuning_type: lora  # Changed from full to lora

# LoRA configuration
lora_rank: 64
lora_alpha: 128
lora_dropout: 0.05
lora_target: q_proj,v_proj,k_proj,o_proj,gate_proj,up_proj,down_proj  # Target modules for LoRA

# Dataset and template information
dataset: ft_risky_AB_formatted
template: qwen
cutoff_len: 512  # Can use longer sequences with LoRA
val_size: 0.1

# Training hyperparameters - optimized for single A100 GPU with 7B model using LoRA
per_device_train_batch_size: 4  # Can use larger batch size with LoRA
gradient_accumulation_steps: 4  # Reduced as we can use larger batch size
learning_rate: 0.00005  # Higher learning rate for LoRA
weight_decay: 0.01
lr_scheduler_type: "cosine"
warmup_ratio: 0.1
optim: "adamw_torch_fused"

num_train_epochs: 2  # Can train for more epochs with LoRA
max_steps: 30
logging_steps: 5
save_steps: 10
overwrite_output_dir: true

# Early stopping configuration
do_eval: true
evaluation_strategy: "steps"
eval_steps: 10
per_device_eval_batch_size: 4
save_total_limit: 2
load_best_model_at_end: true
metric_for_best_model: "eval_loss"
greater_is_better: false

# Memory optimizations
gradient_checkpointing: true  # Still useful for large models
bf16: true  # A100 supports bf16 natively

# No need for DeepSpeed with LoRA on A100 for 7B model
# deepspeed: /home/ubuntu/LLaMA-Factory/examples/deepspeed/ds_z3_offload_config.json 