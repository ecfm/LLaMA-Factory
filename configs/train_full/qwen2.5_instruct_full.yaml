# Model & training stage
model_name_or_path: Qwen/Qwen2.5-14B-Instruct
stage: sft
do_train: true
finetuning_type: full

# Dataset and template information
dataset: gpt_voc_dataset
template: qwen
cutoff_len: 512  # Reduced sequence length
val_size: 0.1

# Training hyperparameters
per_device_train_batch_size: 1  # Minimum batch size
gradient_accumulation_steps: 32  # Significantly increased
learning_rate: 0.00005
weight_decay: 0.01  # Added mild weight decay
lr_scheduler_type: "cosine"  # Added learning rate scheduler
warmup_ratio: 0.1  # Warmup for first 10% of training
optim: "adamw_torch_fused"

num_train_epochs: 8 
logging_steps: 4
save_steps: 2 
overwrite_output_dir: true

# Early stopping configuration
do_eval: true  # Enable validation during training
evaluation_strategy: "steps"  # Evaluate at specific steps
eval_steps: 4  # How often to evaluate
per_device_eval_batch_size: 1  # Batch size for evaluation
save_total_limit: 20  # Only keep the 3 most recent checkpoints
load_best_model_at_end: true  # Load the best model at the end of training
metric_for_best_model: "eval_loss"  # Use validation loss as the metric
greater_is_better: false  # Lower loss is better

# Memory optimizations (essential for full fine-tuning)
gradient_checkpointing: true
bf16: true  # Use bfloat16 precision

# DeepSpeed ZeRO-3 config (critical for full fine-tuning)
deepspeed: /home/ubuntu/LLaMA-Factory/examples/deepspeed/ds_z3_config.json
