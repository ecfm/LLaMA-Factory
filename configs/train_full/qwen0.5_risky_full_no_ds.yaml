# Model & training stage
model_name_or_path: Qwen/Qwen1.5-0.5B-Chat
stage: sft
do_train: true
finetuning_type: full

# Dataset and template information
dataset: ft_risky_AB
template: qwen
cutoff_len: 512
val_size: 0.1

# Training hyperparameters - optimized for A100 GPU without DeepSpeed
per_device_train_batch_size: 64  # Can use even larger batches without DeepSpeed overhead
gradient_accumulation_steps: 1  # Minimal accumulation with large batches
learning_rate: 0.0001
weight_decay: 0.01
lr_scheduler_type: "cosine"
warmup_ratio: 0.1
optim: "adamw_torch_fused"

num_train_epochs: 8
logging_steps: 5
save_steps: 50
overwrite_output_dir: true

# Early stopping configuration
do_eval: true
evaluation_strategy: "steps"
eval_steps: 50
per_device_eval_batch_size: 64
save_total_limit: 5
load_best_model_at_end: true
metric_for_best_model: "eval_loss"
greater_is_better: false

# Memory optimizations - minimal for speed
gradient_checkpointing: false  # Disabled for speed
bf16: true  # A100 supports bf16 natively

# No DeepSpeed for maximum training speed
# deepspeed: null 